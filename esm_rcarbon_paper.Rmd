---
title: "Inferences from large sets of radiocarbon dates: software and method - Electronic Supplementary Material"
author: "E.Crema"
date: "9 April 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This document includes R scripts for generating all figures in the article _Inferences from large sets of radiocarbon dates: software and method_, as well as supplementary figures and their scripts.    

# Setup

All analyses and figures require the installation of the _rcarbon_ package version <!-- specify version here --> and the download of several supporting data. To install _rcarbon_ type the following command in the R console:

```{r,eval=FALSE}
# install.packages('devtools') ## execute the line if devtools is not installed
devtools::install_github("ahb108/rcarbon")
```

<!-- In case the package in submitted to CRAN -->
<!-- ```{r,eval=FALSE}
install.packages("rcarbon")
``` -->

Once the package is downloaded and installed, load _rcarbon_ by typing:

```{r}
library(rcarbon)
```

To reproduce the figures in the article the following data should be imported or loaded in R:

<!-- Perhaps worth creating two sections, one for the figures in the article and one for the supplementary figures -->
<!-- AB: Or we can put them all together...presumably we will just submit a zip file with the raw data and Rmd file, but no html/pdf?-->

# Main Article Figures 


## Figure 1
<!-- Not yet checked wiht the knitted version -->
```{r,fig.width=6,fig.height=7}
data(euroevol)
grimes <- euroevol[grepl("S2072", euroevol$SiteID),]
grimesc <- calibrate(x=grimes$C14Age, errors=grimes$C14SD, ids=grimes$C14ID, normalised=FALSE)
workingstartBP <- 7500
workingendBP <- 2500
gspd0 <- spd(x=grimesc, timeRange=c(workingstartBP,workingendBP), datenormalised=TRUE)
threedates <- grimes[grimes$LabCode %in% c("BM-1022","BM-1033","BM-1066"),]
threedatesc <- calibrate(x=threedates$C14Age, errors=threedates$C14SD, ids=threedates$C14ID, normalised=FALSE)

## Thinned version
n <- 10
inds <- thinDates(ages=grimes$C14Age,errors=grimes$C14SD,grimes$SiteID,size=n,method="random", seed=99)
grimesthin <- grimes[inds,]
grimesthinc <- calibrate(x=grimesthin$C14Age, errors=grimesthin$C14SD, ids=grimesthin$C14ID, normalised=FALSE)
gspdthin <- spd(x=grimesthinc, timeRange=c(workingstartBP,workingendBP), datenormalised=TRUE)

## Binned version
bins50 <- binPrep(sites=grimes$SiteID, ages=grimes$C14Age, h=50)
bins100 <- binPrep(sites=grimes$SiteID, ages=grimes$C14Age, h=100)
bins200 <- binPrep(sites=grimes$SiteID, ages=grimes$C14Age, h=200)
gspd50 <- spd(x=grimesc, bins=bins50, timeRange=c(workingstartBP,workingendBP), datenormalised=TRUE)
gspd100<- spd(x=grimesc, bins=bins100, timeRange=c(workingstartBP,workingendBP), datenormalised=TRUE)
gspd200 <- spd(x=grimesc, bins=bins200, timeRange=c(workingstartBP,workingendBP), datenormalised=TRUE)

## I think we should add a proper 'add=TRUE' option to the basic plot.CalDates so the fiddling with polygons below is not necessary, it could work only off type=simple, for instance.
dev.new(device=pdf, width=6, height=7)
layout(matrix(c(1,1,2,3,4,5), 3, 2, byrow=TRUE), widths=c(3,3), heights=c(3,2,2))
par(mar=c(4, 4, 1, 0.5)) #c(bottom, left, top, right)
plot(gspd0, calendar="BCAD", fill.p="chocolate1")
plot(gspd0, runm=50, calendar="BCAD", add=TRUE, fill.p=NA, border="chocolate4")
xs <- c(BPtoBCAD(threedatesc$grids[[1]]$calBP), rev(BPtoBCAD(threedatesc$grids[[1]]$calBP)))
ys <- c(threedatesc$grids[[1]]$PrDens, rep(0,length(threedatesc$grids[[1]]$PrDens)))
polygon(xs,ys, col="black", border=NA)
xs <- c(BPtoBCAD(threedatesc$grids[[2]]$calBP), rev(BPtoBCAD(threedatesc$grids[[2]]$calBP)))
ys <- c(threedatesc$grids[[2]]$PrDens, rep(0,length(threedatesc$grids[[2]]$PrDens)))
polygon(xs,ys, col="black", border=NA)
xs <- c(BPtoBCAD(threedatesc$grids[[3]]$calBP), rev(BPtoBCAD(threedatesc$grids[[3]]$calBP)))
ys <- c(threedatesc$grids[[3]]$PrDens, rep(0,length(threedatesc$grids[[3]]$PrDens)))
polygon(xs,ys, col="black", border=NA)
legend("topleft", bty="n", col=c("chocolate1","chocolate4","black"), legend=c("raw SPD", "smoothed (50-yr running mean)", "3 example dates"), lwd=c(5,1,5), cex=0.8)
text(-5400, 0.02, "a", font=2)
par(mar=c(0.5, 3, 0.5, 0.5)) #c(bottom, left, top, right)
plot(gspd0, xaxt="n", yaxt="n", runm=50, fill.p=NA, border="white", spdnormalised=TRUE)
plot(gspdthin, xaxt="n", yaxt="n", runm=50, fill.p="aquamarine", spdnormalised=TRUE, add=TRUE)
plot(gspd0, xaxt="n", yaxt="n", runm=50, fill.p=NA, border="chocolate4", spdnormalised=TRUE, add=TRUE)
legend("topleft", bty="n", col=c("chocolate4","aquamarine"), legend=c("original SPD", paste("Thinned to ", nrow(grimesthin), " random dates", sep="")), lwd=c(1,5), cex=0.8)
text(7200, 0.00015, "b", font=2)
plot(gspd0, xaxt="n", yaxt="n", runm=50, fill.p=NA, border="white", spdnormalised=TRUE)
plot(gspd50, xaxt="n", yaxt="n", runm=50, fill.p="bisque3", spdnormalised=TRUE, add=TRUE)
plot(gspd0, xaxt="n", yaxt="n", runm=50, fill.p=NA, border="chocolate4", spdnormalised=TRUE, add=TRUE)
legend("topleft", bty="n", col=c("chocolate4","bisque3"), legend=c("original SPD", "50 year bins"), lwd=c(1,5), cex=0.8)
text(7200, 0.00015, "c", font=2)
plot(gspd0, xaxt="n", yaxt="n", runm=50, fill.p=NA, border="white", spdnormalised=TRUE)
plot(gspd100, xaxt="n", yaxt="n", runm=50, fill.p="bisque3", spdnormalised=TRUE, add=TRUE)
plot(gspd0, xaxt="n", yaxt="n", runm=50, fill.p=NA, border="chocolate4", spdnormalised=TRUE, add=TRUE)
legend("topleft", bty="n", col=c("chocolate4","bisque3"), legend=c("original SPD", "100 year bins"), lwd=c(1,5), cex=0.8)
text(7200, 0.00015, "d", font=2)
plot(gspd0, xaxt="n", yaxt="n", runm=50, fill.p=NA, border="white", spdnormalised=TRUE)
plot(gspd200, xaxt="n", yaxt="n", runm=50, fill.p="bisque3", spdnormalised=TRUE, add=TRUE)
plot(gspd0, xaxt="n", yaxt="n", runm=50, fill.p=NA, border="chocolate4", spdnormalised=TRUE, add=TRUE)
legend("topleft", bty="n", col=c("chocolate4","bisque3"), legend=c("original SPD", "200 year bins"), lwd=c(1,5), cex=0.8)
text(7200, 0.00015, "e", font=2)
```

## Figure 2

<!-- consider renaming the columns in the CSV for consistency? -->
<!-- consider removing undrscores from brazil sitenames in csv -->
<!-- calibration curve still needed? -->

The code below compares the summed probability distribution (SPD) of three distinct geographic regions (Southern Levant, Sahara, and Brazil) using normalised calibrated dates. 

First we calibrate the dates. For the purpose of this figure no binning will be employed, and the dates are be immediately aggregated to generate the SPDs:

```{r}
# Read Calibration Curve For Visualisation
intcal13 <- read.csv('http://www.radiocarbon.org/IntCal13%20files/intcal13.14c', encoding="UTF-8",skip=11,header=F)
colnames(intcal13) <- c("BP","CRA","Error","D14C","Sigma")

# Roberts, Neil, Jessie Woodbridge, Andrew Bevan, Alessio Palmisano, Stephen Shennan, and Eleni Asouti. “Human Responses and Non-Responses to Climatic Variations during the Last Glacial-Interglacial Transition in the Eastern Mediterranean.” Quaternary Science Reviews, Late Glacial to Early Holocene socio-ecological responses to climatic instability within the Mediterranean basin, 184 (March 15, 2018): 47–67. https://doi.org/10.1016/j.quascirev.2017.09.011.
data(emedyd) # this is available within the rcarbon package
southLevant <- subset(emedyd,Region=='1'&CRA<=17000&CRA>=8000)

#Manning, Katie, and Adrian Timpson. “The Demographic Response to Holocene Climate Change in the Sahara.” Quaternary Science Reviews 101 (October 1, 2014): 28–35. https://doi.org/10.1016/j.quascirev.2014.07.003.
sahara <- read.csv('./data/Manning_and_Timpson2014.csv',header=TRUE)
sahara <- subset(sahara,Yrs.BP<=17000&Yrs.BP>=8000)

#Bueno, Lucas, Adriana Schmidt Dias, and James Steele. “The Late Pleistocene/Early Holocene Archaeological Record in Brazil: A Geo-Referenced Database.” Quaternary International, A Late Pleistocene/early Holocene archaeological 14C database for Central and South America: palaeoenvironmental contexts and demographic interpretations, 301 (July 8, 2013): 74–93. https://doi.org/10.1016/j.quaint.2013.03.042.
brazil <- read.csv('./data/Bueno_etal_2013.csv',header=TRUE)
brazil <- subset(brazil,OccCRA<=17000&OccCRA>=8000)
```
<!-- updated version matching the current version of the manuscript as for 09042019 -->

```{r}
## Calibrate Dates
southlevant.x = calibrate(southLevant$CRA,southLevant$Error,normalised=F,verbose=F)
sahara.x = calibrate(sahara$Yrs.BP,sahara$STD,normalised=F,verbose=F)
brazil.x = calibrate(brazil$CRA,brazil$Error,normalised=F,verbose=F)

## Minimal binning to control for sites with many dates.
southlevant.bins <- binPrep(sites=southLevant$SiteName, ages=southLevant$CRA, h=50)
sahara.bins <- binPrep(sites=sahara$Sitename, ages=sahara$Yrs.BP, h=50)
brazil$SiteName <- sub("_","",brazil$SiteName) ##can we fix this in the csv?
brazil.bins <- binPrep(sites=brazil$SiteName, ages=brazil$CRA, h=50)

## Aggregate 
southlevant.spd.nnorm = spd(southlevant.x, bins=southlevant.bins, timeRange=c(16000,9000),verbose=F)
sahara.spd.nnorm = spd(sahara.x, bins=sahara.bins, timeRange=c(16000,9000),verbose=F)
brazil.spd.nnorm = spd(brazil.x, bins=brazil.bins, timeRange=c(16000,9000),verbose=F)
southlevant.spd.norm = spd(southlevant.x, bins=southlevant.bins, timeRange=c(16000,9000),verbose=F,datenormalised=T)
sahara.spd.norm = spd(sahara.x, bins=sahara.bins, timeRange=c(16000,9000),verbose=F,datenormalised=T)
brazil.spd.norm = spd(brazil.x, bins=brazil.bins, timeRange=c(16000,9000),verbose=F,datenormalised=T)
```
```{r,fig.width=10,fig.height=5}
## Plot
fmat=matrix(c(rep(1,3),rep(2,3),rep(3:5,each=2)),nrow=6,ncol=2)
layout(fmat,widths=c(0.4,0.8),heights=c(1.2,1,0.8,0.8,1,1.2))
x1 = calibrate(3523,45,normalised = F,verbose=F)
plot(x1,type='auc')
#title(paste0("auc non-normalised:",round(sum(x1$grids$`1`$PrDens),3)))
title("a")
x2 = calibrate(4274,45,normalised = F,verbose=F)
plot(x2,type='auc')
#title(paste0("auc non-normalised:",round(sum(x2$grids$`1`$PrDens),3)))
title("b")
par(yaxs="i")
par(xaxs="i")
par(mar=c(0,6,6,1)) # c(bottom, left, top, right)
plot(southlevant.spd.nnorm,fill.p=NA, xlim=c(14000,9000), ylim=c(0,0.25), xaxt="n")
rect(x=c(9509.948,10228.445,11241.710,12690)+100,xright=c(9509.948,10228.445,11241.710,12690)-100,ybottom=rep(-100,4),ytop=rep(100,5),border=NA,col='orange')
plot(southlevant.spd.nnorm,fill.p='bisque3',ylim=c(0,0.25), xaxt="n", add=TRUE)
plot(southlevant.spd.norm,type='simple', xaxt="n",add=TRUE)
lines(intcal13[intcal13$BP<=14000 & intcal13$BP>=9000,"BP"],reScale(intcal13[intcal13$BP<=14000 & intcal13$BP>=9000,"CRA"])*par('usr')[4],col="red",lwd=2)
legend("topleft",legend=c("Unnormalised SPD","Normalised SPD", "IntCal13","Areas with spikes"),col=c('bisque3','black',"red","orange"),lwd=c(5,1,1,5))
title("c")
par(mar=c(0,6,0,1))
plot(sahara.spd.nnorm,fill.p=NA, xlim=c(14000,9000), ylim=c(0,0.35), xaxt="n")
rect(x=c(9509.948,10228.445,11241.710,12690)+100,xright=c(9509.948,10228.445,11241.710,12690)-100,ybottom=rep(-100,4),ytop=rep(100,5),border=NA,col='orange')
plot(sahara.spd.nnorm,fill.p='bisque3',ylim=c(0,0.35), xaxt="n", add=TRUE)
plot(sahara.spd.norm,type = 'simple', xaxt="n",add=TRUE)
lines(intcal13[intcal13$BP<=14000 & intcal13$BP>=9000,"BP"],reScale(intcal13[intcal13$BP<=14000 & intcal13$BP>=9000,"CRA"])*par('usr')[4],col="red",lwd=2)
title("d")
par(mar=c(6,6,0,1))
plot(brazil.spd.nnorm,fill.p=NA, xlim=c(14000,9000), ylim=c(0,0.125), xaxt="n")
rect(x=c(9509.948,10228.445,11241.710,12690)+100,xright=c(9509.948,10228.445,11241.710,12690)-100,ybottom=rep(-100,4),ytop=rep(100,5),border=NA,col='orange')
plot(brazil.spd.nnorm,fill.p='bisque3',ylim=c(0,0.125), xaxt="n", add=TRUE)
plot(brazil.spd.norm,type = 'simple', xaxt="n", add=TRUE)
axis(side=1, at=seq(14000,9000,-1000), labels=seq(14000,9000,-1000), las=2, cex.axis=0.7)
lines(intcal13[intcal13$BP<=14000 & intcal13$BP>=9000,"BP"],reScale(intcal13[intcal13$BP<=14000 & intcal13$BP>=9000,"CRA"])*par('usr')[4],col="red",lwd=2)
title("e")
```


<!-- The script below is deprecated -->
## Figure 2 

The code below compares the summed probability distribution (SPD) of three distinct geographic regions (Southern Levant, Sahara, and Brazil) using normalised and unnormalised calibrated dates. 

```{r}
southlevant.x = calibrate(southLevant$CRA,southLevant$Error,normalised=T,verbose=F)
southlevant.spd = spd(southlevant.x,timeRange=c(16000,9000),verbose=F)

sahara.x = calibrate(sahara$Yrs.BP,sahara$STD,normalised=T,verbose=F)
sahara.spd = spd(sahara.x,timeRange=c(16000,9000),verbose=F)

brazil.x = calibrate(brazil$CRA,brazil$Error,normalised=T,verbose=F)
brazil.spd = spd(brazil.x,timeRange=c(16000,9000),verbose=F)
```

We then visualise the SPD justaxaposing them to the calibration curve. The script below highlights time intervals where the curve is steeper and hence more likely to cause peaks in the SPD:

```{r,fig.height=9,fig.width=4}
par(mfrow=c(3,1))
plot(southlevant.spd,fill.p=NA)
rect(x=c(9509.948,10228.445,11241.710)+100,xright=c(9509.948,10228.445,11241.710)-100,ybottom=rep(-100,4),ytop=rep(100,4),border=NA,col='orange')
plot(southlevant.spd,add=TRUE,fill.p='grey17')
abline(v=c(9509.948,10228.445,11241.710)+100,lty=3,col='orange') #peak locations
abline(v=c(9509.948,10228.445,11241.710)-100,lty=3,col='orange') #peak locations
title(paste0("South Levant (n=",length(southlevant.x),")"))
par(new=T)
plot(intcal13$BP,intcal13$CRA,col="indianred",lwd=2,axes=F,xlab="",ylab="",type="l",ylim=c(8055,13295),xlim=c(16000,9000))

plot(sahara.spd,fill.p=NA)
rect(x=c(9509.948,10228.445,11241.710)+100,xright=c(9509.948,10228.445,11241.710)-100,ybottom=rep(-100,4),ytop=rep(100,4),border=NA,col='orange')
plot(sahara.spd,add=TRUE,fill.p='grey17')
abline(v=c(9509.948,10228.445,11241.710)+100,lty=3,col='orange') #peak locations
abline(v=c(9509.948,10228.445,11241.710)-100,lty=3,col='orange') #peak locations
title(paste0("Sahara (n=",length(sahara.x),")"))
par(new=T)
plot(intcal13$BP,intcal13$CRA,col="indianred",lwd=2,axes=F,xlab="",ylab="",type="l",ylim=c(8055,13295),xlim=c(16000,9000))


plot(brazil.spd,fill.p=NA)
rect(x=c(9509.948,10228.445,11241.710)+100,xright=c(9509.948,10228.445,11241.710)-100,ybottom=rep(-100,4),ytop=rep(100,4),border=NA,col='orange')
plot(brazil.spd,add=TRUE,fill.p='grey17')
title(paste0("Brazil (n=",length(brazil.x),")"))
abline(v=c(9509.948,10228.445,11241.710)+100,lty=3,col='orange') #peak locations
abline(v=c(9509.948,10228.445,11241.710)-100,lty=3,col='orange') #peak locations
par(new=T)
plot(intcal13$BP,intcal13$CRA,col="indianred",lwd=2,axes=F,xlab="",ylab="",type="l",ylim=c(8055,13295),xlim=c(16000,9000))
```




<!-- two possible options for figure 2 below -->

<!-- option A-->
## Figure 2

The script below shows the use of rolling mean and composite kernel density estimate (CKDE) to reduce the effect of artifical spikes in the SPD of the southern Levant dataset. 

For the CKDE we need to first generate `nsim` random calendar dates from each calibrated probability distributions. We then feed this into the the `ckde()` function which generates a kernel density estimate from each simulation set. The resulting object can then be plotted with the standard `plot()` function.

```{r}
sl.dates=sampleDates(southlevant.x,nsim=1000,verbose=FALSE)
ckde100=ckde(sl.dates,timeRange = c(16000,9000),bw = 100)
ckde200=ckde(sl.dates,timeRange = c(16000,9000),bw = 200)
ckde300=ckde(sl.dates,timeRange = c(16000,9000),bw = 300)
```

The plot method for `CalSPD` class objects (the output of the `spd()` function) has a dedicated argument `runm` which enables users to specify the window size of the moving average. 

```{r,fig.width=10,fig.height=5}
# Moving Windows with 100,200 and 300 years:
par(mfrow=c(2,3))
plot(southlevant.spd,runm=100)
title("SPD (100 yrs rolling average)")
plot(southlevant.spd,runm=200)
title("SPD (200 yrs rolling average)")
plot(southlevant.spd,runm=300)
title("SPD (300 yrs rolling average)")

# CKDE with 100,200, 300 years bandwidth:
plot(ckde100,main="CKDE (100 years bandwidth)")
plot(ckde200,main="CKDE (200 years bandwidth)")
plot(ckde300,main="CKDE (300 years bandwidth)")
```


# Figure 2b
<!-- option 2: just show CKDE -->

The _rcarbon_ package requires two steps for generating CKDE (Composite Kernel Density Estimates). First we generate `nsim` random calendar dates from each calibrated probability distributions. We then feed this into the the `ckde()` function which computes a kernel density estimate from each simulation set with the user-defined bandwidth. The resulting object can then be plotted with the standard `plot()` function.

```{r,fig.height=9,fig.width=4}
sl.dates=sampleDates(southlevant.x,nsim=1000,verbose=FALSE)
ckde100=ckde(sl.dates,timeRange = c(16000,9000),bw = 100)
ckde200=ckde(sl.dates,timeRange = c(16000,9000),bw = 200)
ckde300=ckde(sl.dates,timeRange = c(16000,9000),bw = 300)

par(mfrow=c(3,1))
plot(ckde100,type='multiline',main="CKDE South Levant (100 years bandwidth)")
plot(ckde200,type='multiline',main="CKDE South Levant (200 years bandwidth)")
plot(ckde300,type='multiline',main="CKDE South Levant (300 years bandwidth)")
```




# Figure 3

The figure below compares the normalised (dotred line) and non-normalised (filled polygon) calibrated distributions. The area under the curve (auc) for the normalised distribution is equal to 1 in both cases, but the auc for non-normalised distribution is >1 for flatter portions of the calibration curve (left panel) and <1 for steeper portions of the calibration curve. 

```{r,fig.width=7,fig.height=4}
par(mfrow=c(1,2))
x1 = calibrate(3523,45,normalised = F,verbose=F)
plot(x1,type='auc',label='a')
title(paste0("auc non-normalised:",round(sum(x1$grids$`1`$PrDens),3)))
x2 = calibrate(4274,45,normalised = F,verbose=F)
plot(x2,type='auc',label='b')
title(paste0("auc non-normalised:",round(sum(x2$grids$`1`$PrDens),3)))
```


# Figure 4

The cumulative impact of normalised and normalised dates can be evaluated by comparing the SPDs resulting from each approach to calibration. The code below does this for each of the three datasets showed in figure 1. 

<!-- alternatively just show the emedyd -->

```{r}
southlevant.x2 = calibrate(southLevant$CRA,southLevant$Error,normalised=F,verbose=F)
southlevant.spd2 = spd(southlevant.x2,timeRange=c(16000,9000),verbose=F)
sahara.x2 = calibrate(sahara$Yrs.BP,sahara$STD,normalised=F,verbose=F)
sahara.spd2 = spd(sahara.x2,timeRange=c(16000,9000),verbose=F)
brazil.x2 = calibrate(brazil$CRA,brazil$Error,normalised=F,verbose=F)
brazil.spd2 = spd(brazil.x2,timeRange=c(16000,9000),verbose=F)
```

```{r,fig.height=9,fig.width=4}
par(mfrow=c(3,1))
plot(southlevant.spd,spdnormalised=TRUE,fill.p='grey55')
plot(southlevant.spd2,spdnormalised=TRUE,lwd=2,add=TRUE,type = 'simple')
title("Southern Levant")
plot(sahara.spd,spdnormalised=TRUE,fill.p = 'grey55')
plot(sahara.spd2,spdnormalised=TRUE,add=TRUE,lwd=2,type = 'simple')
title("Sahara")
plot(brazil.spd,spdnormalised=TRUE,fill.p = 'grey55')
plot(brazil.spd2,spdnormalised=TRUE,add=TRUE,lwd=2,type = 'simple')
title("Brazil")
legend("topleft",legend=c("Normalised","Not-Normalised"),col=c("grey55",1),lwd=c(4,2))
```


# Figure 5

The figure below compares an SPD based on  dates calibrated without normalisation (a) to a curve generated by calibrating the sum of gaussian curves based on the CRAs and associated errors. 


```{r}
data(emedyd)
d=subset(emedyd,Region==1&CRA<10000&CRA>7000)

# Create SPD using uncalibrated dates
sumUncalMat=matrix(NA,nrow=length(13000:4000),ncol=nrow(d))
for (i in 1:nrow(d))
{
 sumUncalMat[,i]=dnorm(13000:4000,mean=d$CRA[i],sd=d$Error[i])
}

PrDensNN = apply(sumUncalMat,1,sum)
PrDens = PrDensNN/sum(PrDensNN)
sumUncal=data.frame(CRA=13000:4000,PrDens=PrDens)

# Calibrate the entire uncalibrated SPD 
sumCal=calibrate(as.UncalGrid(sumUncal),eps=0)

# Normalise Probability Sum to Unity
sumCal$PrDens=sumCal$PrDens/sum(sumCal$PrDens)

# Calibrate dates without normalisation
x=calibrate(d$CRA,d$Error,normalised=F)
# Create SPD
SPDnn=spd(x,timeRange=c(12000,8000),spdnormalised=T)
# Plot and Compare
plot(SPDnn)
lines(sumCal$calBP,sumCal$PrDens,col="red",type="l",lwd=1,lty=2)
#legend("bottom",bg="white",legend=c("a","b"),col=c("lightgrey","red"),lwd=c(8,1),lty=c(1,2))
```



# Figure 6

The script below compares OxCal's SUM function (executed via the _oxcAAR_ package) with rcarbon's `spd()` function. 

```{r}
 ## oxcal via oxcAAR
library(oxcAAR)
quickSetupOxcal()
my_dates <- R_Date(southLevant$LabID, southLevant$CRA, southLevant$Error)
my_sum <- oxcal_Sum(my_dates)
my_result_file <- executeOxcalScript(my_sum)
my_result_text <- readOxcalOutput(my_result_file)
my_result_data <- parseFullOxcalOutput(my_result_text)
res=my_result_data[nrow(southLevant)+3]

n=length(res$`ocd[1]`$likelihood$prob) #number of probabilities
PrDens=res$ocd[[6]][[8]]
resolution=res$ocd[[6]][[7]]
PrDens=PrDens/sum(PrDens)/resolution
calBPs = res$ocd[[6]][[6]]+seq(from=0,by=resolution,length.out=n)-1950

 ## Compute SPD in rcarbon using a wider timeRange to control edge effect
southlevant.spd3 = spd(southlevant.x,verbose=F,timeRange=c(21000,8000))

 ## Compare rcarbon and oxcal
plot(southlevant.spd3,spdnormalised=T,xlim=c(21000,8000),fill.p = 'grey55')
lines(abs(calBPs), PrDens,col="red",lty=1,lwd=1.5)
legend("topleft",legend=c("rcarbon (normalised)","OxCal"),col=c("grey55","red"),lwd=c(15,1.5),lty=c(1,1))
```


# Figure 7

The Bin-sensitivity analysis enables a visual inspection of multiple SPDs generated using different values for the parameter `h` in the binning function (`binPrep()`). High values of `h` will aggregate dates that are futher apart in time. The example below is a subset of Danish dates from the EUROEVOL database, with the binning carried out using median calibrated dates.   

```{r,fig.height=4,fig.width=8}
data(euroevol)
denmark = subset(euroevol,Country=='Denmark'&C14Age<=8000&C14Age>3500)
denmark.x = calibrate(denmark$C14Age,denmark$C14SD,normalised=F,verbose=F)
binsense(x=denmark.x,y=denmark$SiteID,timeRange=c(8000,4000),verbose = F,h=seq(0,200,20),runm=200)
```



# Figure 8 

The `calsample` method implemented using normalised and non-normalised dates. The method fails to emulate peaks in the SPD corresponding to the steeper portions of the calibration curve. 

```{r}
test.cal.nn = modelTest(southlevant.x2,errors=southLevant$Error,timeRange=c(16000,9000), nsim=1000, ncores=3, method='calsample', normalised = FALSE,verbose=FALSE)

test.cal.n = modelTest(southlevant.x,errors=southLevant$Error,timeRange=c(16000,9000), nsim=1000, ncores=3, method='calsample', normalised = TRUE,verbose=FALSE)
```

```{r,fig.width=10,fig.height=4}
par(mfrow=c(1,2))
plot(test.cal.n,lwd.obs = 1.5)
title('Calsample (Normalised Dates)')
plot(test.cal.nn,lwd.obs = 1.5)
title('Calsample (Non-Normalised Dates)')
```

# Figure 9
The `uncalsample` method implemented using normalised and non-normalised dates. The method emulates the peaks in the SPD corresponding to the steeper portions of the calibration curve. 

```{r}
test.uncal.nn = modelTest(southlevant.x2,errors=southLevant$Error,timeRange=c(16000,9000), nsim=1000, ncores=3, method='uncalsample', normalised = FALSE,verbose=FALSE)

test.uncal.n = modelTest(southlevant.x,errors=southLevant$Error,timeRange=c(16000,9000), nsim=1000, ncores=3, method='uncalsample', normalised = TRUE,verbose=FALSE)
```

```{r,fig.width=10,fig.height=4}
par(mfrow=c(1,2))
plot(test.uncal.n,lwd.obs = 1.5)
title('Uncalsample (Normalised Dates)')
plot(test.uncal.nn,lwd.obs = 1.5)
title('Uncalsample (Non-Normalised Dates)')
```


# Figure 10

An example of mark permutation test. The simulation envelope represents in this case the panregional model, notice how the width of the simulation envelope is positively correlated to the number of bins in each region.


```{r,fig.height=9,fig.width=4}
# prepare data and generate the panregional spd
data(emedyd)
alldates.emedyd = calibrate(x=emedyd$CRA, errors=emedyd$Error, calCurves='intcal13', normalised=FALSE,verbose=FALSE)
bins = binPrep(sites=emedyd$SiteName, ages=emedyd$CRA, h=50)
alldates.spd = spd(alldates.emedyd,bins=bins,timeRange=c(16000,9000),runm=200,spdnormalised=T,verbose=F)

#execute the permutation test
result = permTest(x=alldates.emedyd, bins=bins, marks=emedyd$Region, timeRange=c(16000,9000), runm=200,nsim=1000,spdnormalised=T,verbose=FALSE)

# plot results
par(mfrow=c(3,1))
plot(result,focalm='1',lwd=2,main=paste0("Southern Levant (n.bins=",result$metadata[[1]][2],")"))
plot(alldates.spd,add=T,type='simple',lty=2,col='grey27')
plot(result,focalm='2',lwd=2,main=paste0("Northern Levant (n.bins=",result$metadata[[2]][2],")"))
plot(alldates.spd,add=T,type='simple',lty=2,col='grey27')
plot(result,focalm='3',lwd=2,main=paste0("South-central Anatolia (n.bins=",result$metadata[[3]][2],")"))
plot(alldates.spd,add=T,type='simple',lty=2,col='grey27')
```

# Figure 11 
```{r,fig.height=2.5,fig.width=8}
## Example with a subset of English and Welsh dates from the Euroevol dataset
data(ewdates)
data(ewowin)
x <- calibrate(x=ewdates$C14Age, errors=ewdates$C14SD, normalised=FALSE)
## Create centennial timeslices (also with site binning)
bins1 <- binPrep(sites=ewdates$SiteID, ages=ewdates$C14Age, h=50)
stkde1 <- stkde(x=x, coords=ewdates[,c("Eastings", "Northings")], win=ewowin, sbw=40000, cellres=2000, focalyears=seq(6500, 5000, -100), tbw=100, bins=bins1, backsight=200, outdir="im",amount=1)
## Plot an example of all four basic outputs for 6000 calBP
par(mar=c(0.5, 0.5, 2.5, 2))
plot(stkde1, 6000, type="all")
```


# Figure 12 

```{r,fig.height=5,fig.width=8}
library(maptools)
data(ewdates)
x <- calibrate(x=ewdates$C14Age, errors=ewdates$C14SD, normalised=FALSE)
bins1 <- binPrep(sites=ewdates$SiteID, ages=ewdates$C14Age, h=50)

# Create SpatialPoints
sites <- unique(data.frame(id=ewdates$SiteID,east=ewdates$Eastings,north=ewdates$Northings))
rownames(sites) <- sites$id
sites <- sites[,-1]
sp::coordinates(sites) <- c("east","north")
sp::proj4string(sites) <- sp::CRS("+init=epsg:27700")

#Compute distance matrix
d <- sp::spDists(sites,sites,longlat=FALSE)
#Compute spatial weights 
w <- spweights(d/1000,h=40)
# Define blocks
breaks <- seq(6300,5900,-200) 

# Spatial Permutation Test
res <- sptest(calDates=x,bins=bins1,timeRange=c(6300,5900),locations=sites,permute="locations",nsim=1000,breaks=breaks,spatialweights=w,ncores=1) 
# Plot Results
par(mar=c(1,1,1,1),mfrow=c(1,2))
base=as(ewowin,"SpatialPolygons")
sp::proj4string(base) <- sp::CRS("+init=epsg:27700")
xlim = bbox(base)[1,]
ylim = bbox(base)[2,]
xlim[1] = xlim[1]-100000
plot(base,col="antiquewhite3",border="antiquewhite3",xlim=xlim,ylim=ylim)
plot(res,index=1,option="raw",add=TRUE,legend=TRUE,legSize=0.8,location="topleft",baseSize=0.9)
plot(base,col="antiquewhite3",border="antiquewhite3",xlim=xlim)
plot(res,index=1,option="test",add=TRUE,legend=TRUE,legSize=0.8,location="topleft",baseSize=0.9)
```


# Supplementary Figures

<!-- move here code and scripts of supplementary figures -->


